{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This program will examine the different factors that are related to a student's test score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, figure\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data frame\n",
    "students=pd.read_csv(os.path.join(os.getcwd(), \"StudentsPerformance.csv\"))\n",
    "avgScores=pd.concat([students[\"math score\"], students[\"reading score\"], students[\"writing score\"]], axis=1)\n",
    "students['mean'] = avgScores.mean(axis=1)#This line is from https://stackoverflow.com/questions/33750326/compute-row-average-in-pandas\n",
    "students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Examining the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before going any further, it would be wise to examine the distribution of test scores in each subject. \n",
    "## The most common test score in math was 65%, in reading 72%, and in writing 74%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "math_hist,reading_hist,writing_hist=students['math score'].hist(color=\"#8B0000\",ax=axs[0]), students['reading score'].hist(color=\"#006400\",ax=axs[1]),students['writing score'].hist(ax=axs[2])\n",
    "\n",
    "math_hist.set_xlabel(\"Math Score\")\n",
    "reading_hist.set_xlabel(\"Reading Score\")\n",
    "writing_hist.set_xlabel(\"Writing Score\")\n",
    "\n",
    "math_hist.grid(None)\n",
    "reading_hist.grid(None)\n",
    "writing_hist.grid(None)\n",
    "\n",
    "fig.suptitle(\"Frequency of Test Scores in Different Subjects\", fontsize=14)\n",
    "plt.subplots_adjust(wspace=.28) \n",
    "fig.text(0.04, 0.5, 'Frequency', rotation=\"vertical\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we know the distribution of test scores, we can begin looking at the factors that affect them. For starters, we can view the relationship between a parent's level of education and their child's test scores.\n",
    "## When a parent has gone to college, especially when they earn a degree, the student's test score increases across subjects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def catPlotMaker(subject,xcor):  \n",
    "    edu=sns.catplot(x=xcor, y=subject, kind=\"bar\", data=students, aspect=2)\n",
    "    edu.ax.set_title(xcor.title()+\" Versus \" + subject.capitalize() +  \" Scores\")\n",
    "    edu.ax.set_xlabel(xcor.title())\n",
    "    edu.ax.set_ylabel(\"Average Score\")\n",
    "    return (edu)\n",
    "\n",
    "catPlotMaker (\"math score\",\"parental level of education\")\n",
    "catPlotMaker (\"reading score\",\"parental level of education\")\n",
    "catPlotMaker (\"writing score\",\"parental level of education\")\n",
    "#These cat plots are not mapped onto subplots, as cat plots do not allow for assigning target axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we can examine the relationship between the type of lunch a student consumes and their test store. \n",
    "## Students with standard lunches on average tend to do better across the board compared to their peers with free/reduced lunches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard=students[students[\"lunch\"]==\"standard\"]\n",
    "free_reduced=students[students[\"lunch\"]==\"free/reduced\"]\n",
    "plt.subplots(1, 3, sharey=True, figsize=(15,5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(\n",
    "   [\"Standard\", \"Free/Reduced\"],[standard[\"math score\"].mean(), free_reduced[\"math score\"].mean()],\n",
    "    color=[\"#8B0000\",\"indigo\"]) #This creates a bar plot displaying the average math test score for students with free/reduced versus standard lunches.\n",
    "plt.xlabel('Math Scores')\n",
    "plt.ylabel('Test Score')\n",
    "plt.subplot(1, 3, 2) #This tells the program which of the three columns to plot the next graph on. \n",
    "plt.bar(\n",
    "   [\"Standard\", \"Free/Reduced\"],[standard[\"reading score\"].mean(), free_reduced[\"reading score\"].mean()],\n",
    "    color=[\"#8B0000\",\"indigo\"])\n",
    "plt.xlabel('Reading Scores')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(\n",
    "   [\"Standard\", \"Free/Reduced\"],[standard[\"writing score\"].mean(), free_reduced[\"writing score\"].mean()],\n",
    "    color=[\"#8B0000\",\"indigo\"])\n",
    "plt.xlabel('Writing Scores')\n",
    "#The code above is from https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html\n",
    "plt.suptitle(\"Type of Lunch vs Test Scores\", fontsize=14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lastly, we can look at the relationship between race/ethnicity and gender versus test scores.\n",
    "## For gender, the relationship depends on the subject. In math, males tend to do better on average, but in writing and reading, females tend to get higher scores. \n",
    "## For race/ethnicity, students from groups D and E tend to do the best, students from group A tend to do the worst, and students from groups B and C are in the middle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxPlot(ycor,title,num):\n",
    "    subject_BP=sns.boxplot(x=\"race/ethnicity\",y=ycor,hue=\"gender\",data=students, ax=axs[num])#This creates a where ycor is the subject being examined and num is the position of axis being plotted on.\n",
    "    subject_BP.axes.set_title(title,fontsize=50)\n",
    "    subject_BP.set_xlabel(\"Race/Ethnicity\",fontsize=30)\n",
    "    subject_BP.set_ylabel(\"Average Score\",fontsize=30)\n",
    "    subject_BP.tick_params(labelsize=30)\n",
    "    #The code above iis from https://stackoverflow.com/questions/36220829/fine-control-over-the-font-size-in-seaborn-plots-for-academic-papers/36222162\n",
    "    plt.setp(subject_BP.get_legend().get_texts(), fontsize='32') #This sets the legend's size.\n",
    "    plt.setp(subject_BP.get_legend().get_title(), fontsize='42') #This sets the title's size.\n",
    "    #The code above is from from https://stackoverflow.com/questions/44880444/how-to-increase-the-font-size-of-the-legend-in-my-seaborn-plot. \n",
    "    return subject_BP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(41,40))\n",
    "boxPlot(\"math score\", \n",
    "        \"Race/Ethnicity & Gender vs Math Test Scores\",0)\n",
    "boxPlot(\"reading score\", \n",
    "        \"Race/Ethnicity & Gender vs Reading Test Scores\",1)\n",
    "boxPlot(\"writing score\", \n",
    "        \"Race/Ethnicity & Gender vs Writing Test Scores\",2)\n",
    "sns.set(rc={'figure.figsize':(41.7,40.27)}) #This sets the size of the figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will be using decision trees to make predictions about how well a student will do given specific circumstances based on the data. \n",
    "    Most of the following code and documentation I will be using is from a webinar hosted by StatQuest: https://www.youtube.com/watch?v=q90UDEgYqeI&ab_channel=StatQuestwithJoshStarmer\n",
    "    \n",
    "    While the code and documentation is largely similar, I applied it to a different dataset from that of the video and made edits when necessary to fit my dataset. For instance, my initial decision tree was much larger than that of StatQuest. As a result, I changed the code to make the program download the image so that the user can zoom in and read the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=students.drop([\"math score\",'reading score','writing score','mean'], axis=1) #Making a new copy of the columns that will be used to make predictions\n",
    "y=students[\"mean\"].copy()# Making a new copy of the column of data I will try to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will now be using One-Hot Encoding to categorize the data. \n",
    "X_encoded=pd.get_dummies(X,columns=[\"race/ethnicity\",\"parental level of education\"])\n",
    "\n",
    "#I will be using a label encoder for items like lunch, gender, etc. where there are only two values. The label encoder will simply give a binary value for the categories.\n",
    "LE=LabelEncoder() #Creating a LabelEncoder object and using that to encode test preparation, gender, and lunch.\n",
    "X_encoded[\"test preparation course\"]=LE.fit_transform(students[\"test preparation course\"])\n",
    "X_encoded['gender'] = LE.fit_transform(students['gender'])\n",
    "X_encoded['lunch'] = LE.fit_transform(students['lunch'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will now encode the y values, assigning a binary value based on whether each student's test score is above or below 75%.\n",
    "above=y>=70\n",
    "below=y<70\n",
    "y[above]=1\n",
    "y[below]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will now create a preliminary classification tree.\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_encoded, y) #Spliting the data into training and testing datasets.\n",
    "clf_dt=DecisionTreeClassifier() #Creating a decision tree classifier object.\n",
    "clf_dt=clf_dt.fit(X_train,y_train)#Fitting the training data to the decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the size of the decision tree, a copy of the output will be saved as a .png file in the same directory as this notebook file. It is recommended that the viewer opens the .png file to zoom in and read the tree. \n",
    "\n",
    "### Key to the decision tree:\n",
    "    Lunch<0.5=Free/Reduced Lunch, Lunch>0.5=Standard Lunch\n",
    "    Test Prepartion<0.5=Completed, Test Preparation>0.5=None\n",
    "    Gender<0.5=Female, Gender>0.5=Male\n",
    "    Race/Ethnicity<0.5=Does not belong to that race/ethnicity, Test Preparation>0.5=Does belong to that race/ethnicity\n",
    "    Parental Level of Education<0.5=Parent is not at this level, Parental Level of Education>0.5=Parent is at this level \n",
    "\n",
    "### The dataset will be split into two sections: training and testing data. The program will use the training data to create the decision tree and the testing data to observe its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15.5), dpi=1000) #Creating a figure to plot the decision tree on\n",
    "out = tree.plot_tree(clf_dt, #Plotting the decision tree\n",
    "                    filled=True,\n",
    "                    rounded=True,\n",
    "                    class_names=[\"Above 70\", \"Below 70\"],\n",
    "                    feature_names=X_encoded.columns) \n",
    "#The code I used below for changing the attributes of the lines is from https://stackoverflow.com/questions/62318367/decision-tree-edges-branches-so-light-that-are-invisible.\n",
    "for o in out:\n",
    "    arrow = o.arrow_patch\n",
    "    if arrow is not None:\n",
    "        arrow.set_edgecolor('black')\n",
    "        arrow.set_linewidth(.25)\n",
    "plt.savefig('Preliminary Decision Tree') #Saving the tree as an image file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since this decision tree is based on the training data, I will now use it on the testing data to see how accurate it is at making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, CM_ax = plt.subplots(figsize=(15, 15))\n",
    "sns.set(font_scale=2)\n",
    "CM=plot_confusion_matrix(clf_dt,X_test,y_test,display_labels=[\"Above 70\", \"Below 70\"],ax=CM_ax) #Creating a confusion matrix based on its predictions.\n",
    "plt.grid(False)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.ylabel(\"True Label\",fontsize=20)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=20)\n",
    "sns.set(font_scale=2)\n",
    "TP=CM.confusion_matrix[0][0] #Obtaining the number of true positives.\n",
    "FP=CM.confusion_matrix[0][1] #Obtaining the number of false positives.\n",
    "TN=CM.confusion_matrix[1][1] #Obtaining the number of true negatives.\n",
    "FN=CM.confusion_matrix[1][0] #Obtaining the number of false negatives.\n",
    "total=TP+FP+TN+FN #Calculating the total\n",
    "accuracy=round(((TP+TN)/total)*100,2);misclassification= round(((FP + FN)/total)*100,2);precision=round((TP/(TP + FP))*100,2);sensitivity=round((TP /( TP + FN))*100,2);specificity=round((TN / (TN + FP))*100,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When put up against the testing dataset, the decision tree does well. However, the model may overfit the training dataset, so I will try pruning leaves to see if that yields better results. \n",
    "    I will do this by comparing decision trees with different numbers of leaves pruned. I will decide which tree is the best by comparing their sum of squared residuals (SSRs). This is a measure of how accurately each decision tree makes predictions with the training dataset. The decision tree with the most leaves (our current tree) will do best, as it was created using the training set. However, as aforementioned, this tree may have overfitted the training dataset which means that it may perform worse than trees with fewer leaves (and less fitted to the training dataset) when put up against the testing dataset. This is a case of sacrificing generalizability for accuracy. To avoid this issue, I will apply the value alpha to the SSR of each decision tree. Alpha is a 'penalty' based on the number of leaves each tree has. Alpha raises the SSR of a tree. As a result, the more leaves it has (IE the more fitted the tree is to the training dataset), the bigger the 'penalty' will be, and consequently the higher the SSR (a worse score) the tree will receive. However, we do not know what the optimal value for alpha is. The code in the next few cells will figure this out.\n",
    "    One finds alpha manually by making a decision tree with all the leaves (our current tree) and calculating its SSR. Pruning the leaves of a decision node will yield a larger SSR with the training dataset, which is why we apply alpha. The user must calculate the minimum value for alpha (the 'penalty') to make our current tree yield a higher SSR than a tree with nodes removed. In other words, the user calculates how high alpha must be to justify pruning the leaves of one node, and then another, and then another until there are no more nodes left. The user will now have a list of values for alpha, and a list of possible decision trees. They will then test decision trees against the testing dataset to see which one has the smallest SSR (best score). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we must gather alpha's values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=clf_dt.cost_complexity_pruning_path(X_train,y_train)\n",
    "ccp_alphas=path.ccp_alphas #ccp=Cost Complexity Pruning\n",
    "ccp_alphas=ccp_alphas[:-1] #Excluding alpha's maximum value, as this would be when there are no nodes left.\n",
    "clf_dts=[] #This list will hold all the decision trees with their values for alpha.\n",
    "#We will now produce decision trees for each of the ccp_alpha values and store them in theh clf_dts array.\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    if ccp_alpha>=0:\n",
    "        clf_dt=DecisionTreeClassifier(ccp_alpha=ccp_alpha) \n",
    "        clf_dt.fit(X_train,y_train)\n",
    "        clf_dts.append(clf_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code below shows how changing the value of alpha affects the accuracy. When the value of alpha (the 'penalty' for more leaves) increases, the size of the decision tree will likely get smaller as more leaves will be pruned (for a more in-depth explanation, please refer to my note above). The decision trees from the clf_dts list above run through the test dataset and their accuracy is then plotted on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores= [clf_dt.score(X_train, y_train) for clf_dt in clf_dts]\n",
    "test_scores=[clf_dt.score(X_test,y_test) for clf_dt in clf_dts]\n",
    "#The two lines above are running the testing and training datasets for each of the decision trees created in the cell above.\n",
    "\n",
    "fig, ax= plt.subplots(figsize=(40, 10))\n",
    "ax.set_xlabel(\"Alpha\")\n",
    "ax.set_ylabel(\"Accuracy\") \n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label='test', drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code is designed to find what is currently believed to be the ideal value for alpha.\n",
    " \n",
    "testLine,trainLine=ax.lines[1].get_ydata(), ax.lines[0].get_ydata()#This obtains the all the y coordinates for the test and training lines above.\n",
    "maxScore=0\n",
    "maxIndex=0\n",
    "for index in range(len(testLine)):\n",
    "    if ((testLine[index]*.75)+(trainLine[index]*.25))>maxScore:#Because the testing data is 75% of the total data and the training data is 25%, I used a weighted average to find the ideal alpha value.\n",
    "        maxScore=((testLine[index]*.75)+(trainLine[index]*.25))\n",
    "        maxIndex=index\n",
    "        curr_ideal_alpha=float(\"{:.10f}\".format(list(ccp_alphas)[index])) #ccp_alphas is a list of all the values of alpha. \n",
    "        #Since it is in the same order as the training and test line (in that the 1st entry in ccp_alphas is the value of alpha for the first point in the training and test lines), I can obtain the curr_ideal_alpha by using the index variable.\n",
    "print (\"Currently, the ideal value for alpha is\", curr_ideal_alpha, \"with a mean accuracy of\",str(maxScore)+\"%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation:\n",
    "    While we have an ideal value for alpha, we should make sure that it is consistently the best value. We verify this by splitting the dataset into 10 combinations of training and testing. For every combination, we will create a decision tree with the training data, and factor in alpha to find the optimal decision tree. Once that is completed, we can run it against the testing dataset to see how accurate it is. The average accuracy across the 10 combinations will be recorded. The graph below will display how accurate a decision tree is on average when factoring in different values for alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, alpha_ax = plt.subplots(figsize=(40, 10))\n",
    "fig.suptitle('Decision Tree Pruning: Accuracy of Predictions vs Alpha', fontsize=35)\n",
    "alpha_loop_values=[]\n",
    "for ccp_alpha in ccp_alphas: #This is going through all of the values of alpha and their respective decision trees. \n",
    "    clf_dt=DecisionTreeClassifier(ccp_alpha=ccp_alpha) \n",
    "    scores=cross_val_score(clf_dt, X_train, y_train, cv=10)#This finds how well each decision tree did on average at making predictions.\n",
    "    alpha_loop_values.append([ccp_alpha,np.mean(scores), np.std(scores)])\n",
    "alpha_results=pd.DataFrame(alpha_loop_values, columns=[\"alpha\",\"mean_accuracy\", \"std\"])\n",
    "alpha_plot=alpha_results.plot(x=\"alpha\",y=\"mean_accuracy\", yerr=\"std\", marker=\"o\",ax=alpha_ax)\n",
    "alpha_plot.set_xlabel(\"Alpha\")\n",
    "alpha_plot.set_ylabel(\"Mean Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_ccp_alpha=alpha_results.sort_values(by=['mean_accuracy'], ascending=False)#Finding the exact optimal value for alpha.\n",
    "accuracy=ideal_ccp_alpha.iloc[0][\"mean_accuracy\"]\n",
    "ideal_ccp_alpha=float(ideal_ccp_alpha.iloc[0][\"alpha\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ideal_ccp_alpha==curr_ideal_alpha:\n",
    "    print (\"It seems as though the initial value for alpha has consistently done the best.\")\n",
    "else:\n",
    "    print (\"It seems as though the initial value for alpha did not consistenlty perform the best. Instead, the value of alpha that, on average, did the best job at predicing is:\", ideal_ccp_alpha,\"with a mean accuracy of\",str(accuracy)+\"%\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have the consistently ideal value for alpha, we can create a decision tree with it and see how it compares to our original tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt_pruned=DecisionTreeClassifier(ccp_alpha=ideal_ccp_alpha)\n",
    "clf_dt_pruned=clf_dt_pruned.fit(X_train,y_train)\n",
    "out = tree.plot_tree(clf_dt_pruned,\n",
    "                    filled=True,\n",
    "                    rounded=True,\n",
    "                    class_names=[\"Above 70\", \"Below 70\"],\n",
    "                    feature_names=X_encoded.columns) \n",
    "#The code I used below for changing the attributes of the lines is from https://stackoverflow.com/questions/62318367/decision-tree-edges-branches-so-light-that-are-invisible.\n",
    "for o in out:\n",
    "    arrow = o.arrow_patch\n",
    "    if arrow is not None:\n",
    "        arrow.set_edgecolor('black')\n",
    "        arrow.set_linewidth(.25)\n",
    "plt.savefig('Pruned Tree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, CM2_ax = plt.subplots(figsize=(15, 15))\n",
    "CM=plot_confusion_matrix(clf_dt_pruned,X_test,y_test,display_labels=[\"Above 70\", \"Below 70\"],ax=CM2_ax)\n",
    "plt.grid(False)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "sns.set(font_scale=2)\n",
    "sns.set(font_scale=2)\n",
    "plt.ylabel(\"True Label\",fontsize=20)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=20)\n",
    "TP=CM.confusion_matrix[0][0];FP=CM.confusion_matrix[0][1];TN=CM.confusion_matrix[1][1];FN=CM.confusion_matrix[1][0]\n",
    "total=TP+FP+TN+FN\n",
    "pruned_accuracy=round(((TP+TN)/total)*100, 2);pruned_misclassification= round(((FP + FN)/total)*100,2);pruned_precision=round((TP/(TP + FP))*100,2);pruned_sensitivity=round((TP /( TP + FN))*100,2);pruned_specificity=round((TN / (TN + FP))*100,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_stats={\"Accuracy\": str(accuracy)+\"%\", \"Misclassification \": str(misclassification)+\"%\", \"Precision \": str(precision)+\"%\", \"Sensitivity \":str(sensitivity)+\"%\", \"Specificity \": str(specificity)+\"%\"}\n",
    "pruned_stats= {\"Accuracy\": str(pruned_accuracy)+\"%\", \"Misclassification \": str(pruned_misclassification)+\"%\", \"Precision \": str(pruned_precision)+\"%\", \"Sensitivity \":str(pruned_sensitivity)+\"%\", \"Specificity \": str(pruned_specificity)+\"%\"}\n",
    "print (\"The original stats were the following: \\n\", original_stats,\"\\n\")\n",
    "print (\"After pruning the tree, the stats become the following: \\n\", pruned_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
